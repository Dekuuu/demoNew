springfox:
  documentation:
    swagger:
      v2:
        path: /api-docs
server:
  port: 9080


  #datasource
spring:
  application:
    name: demo-service
  cloud:
    nacos:
      discovery:
        server-addr: ${NACOS_SERVER}
        namespace: 6d7ab29c-7da9-488c-a319-0f4219de7b1c
        group: dev
  jackson:
    time-zone: GMT+8
    date-format: yyyy-MM-dd HH:mm:ss
  datasource:
    dynamic:
      primary: master
      strict: true
      datasource:
        master:
          #      多数据源时用jdbc-url，单数据源时用url
          driverClassName: com.mysql.jdbc.Driver
          url: jdbc:mysql://${mysql.ip}:${mysql.port}/${mysql.database}?useSSL=false
          username: ${mysql.user}
          password: ${mysql.password}
          initSize: 5
          maxActive: 20
          minIdle: 5
          maxWait: 5000
          testOnReturn: true
          type: com.alibaba.druid.pool.DruidDataSource
        slave1:
          driverClassName: com.mysql.jdbc.Driver
          url: jdbc:mysql://${mysql.ip}:${mysql.port}/${mysql.database.slave}?useSSL=false
          username: ${mysql.user}
          password: ${mysql.password}
          initSize: 5
          maxActive: 20
          minIdle: 5
          maxWait: 5000
          testOnReturn: true
    #============== kafka ==================
  # 指定kafka 代理地址，可以多个
  kafka:
    bootstrap-servers: ${kafka.server}
    producer:
      retries: 5
      batch-size: 16384
      acks: 1
      buffer-memory: 33554432
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:
      group-id: test1-group
      auto-offset-reset: earliest
      enable-auto-commit: false
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 配置消费者的 Json 反序列化的可信赖包，反序列化实体类需要
      properties:
        spring:
          json:
            trusted:
              packages: "*"
      # 这个参数定义了poll方法最多可以拉取多少条消息，默认值为500。如果在拉取消息的时候新消息不足500条，那有多少返回多少；如果超过500条，每次只返回500。
      # 这个默认值在有些场景下太大，有些场景很难保证能够在5min内处理完500条消息，
      # 如果消费者无法在5分钟内处理完500条消息的话就会触发reBalance,
      # 然后这批消息会被分配到另一个消费者中，还是会处理不完，这样这批消息就永远也处理不完。
      # 要避免出现上述问题，提前评估好处理一条消息最长需要多少时间，然后覆盖默认的max.poll.records参数
      # 注：需要开启BatchListener批量监听才会生效，如果不开启BatchListener则不会出现reBalance情况
      max-poll-records: 3
    properties:
      # 两次poll之间的最大间隔，默认值为5分钟。如果超过这个间隔会触发reBalance
      max:
        poll:
          interval:
            ms: 15000
      # 当broker多久没有收到consumer的心跳请求后就触发reBalance，默认值是10s
      session:
        timeout:
          ms: 10000
    listener:
      missing-topics-fatal: false
#      手动提交offset
      ack-mode: manual
      type: batch #开启消费批量拉取
  redis:
    #数据库索引
    database: 0
    host: ${redis.ip}
    port: ${redis.port}
    password:
    jedis:
      pool:
        #最大连接数
        max-active: 8
        #最大阻塞等待时间(负数表示没限制)
        max-wait: -1
        #最大空闲
        max-idle: 8
        #最小空闲
        min-idle: 0
        #连接超时时间
    timeout: 10000

#elasticsearch:
#  user:
#  password:
#  host: 172.25.22.60:9200


feign:
  hystrix:
    enabled: true
# 设置hystrix的超时时间，默认是1000ms
hystrix:
  command:
    default:
      execution:
        isolation:
          thread:
            timeoutInMilliseconds: 10000
#  =============== provider  =======================
#
#  spring.kafka.producer.retries=0
#  # 每次批量发送消息的数量
#  spring.kafka.producer.batch-size=16384
#  spring.kafka.producer.buffer-memory=33554432
#
#  # 指定消息key和消息体的编解码方式
#  spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
#  spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
#
#  #=============== consumer  =======================
#  # 指定默认消费者group id
#  spring.kafka.consumer.group-id=test-hello-group
#
#  spring.kafka.consumer.auto-offset-reset=earliest
#  spring.kafka.consumer.enable-auto-commit=true
#  spring.kafka.consumer.auto-commit-interval=100
#
#  # 指定消息key和消息体的编解码方式
#  spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#  spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

seata:
  # 这里要特别注意和nacos中配置的要保持一致，建议配置成 ${spring.application.name}-tx-group
  tx-service-group: demo_tx_group
  registry:
    type: nacos
    nacos:
      # 配置所在命名空间ID，如未配置默认public空间
      server-addr: ${NACOS_SERVER}
      namespace: 6d7ab29c-7da9-488c-a319-0f4219de7b1c
      group: dev
      application: seata-server
      userName: nacos
      password: nacos
  config:
    type: nacos
    nacos:
      server-addr: ${NACOS_SERVER}
      namespace: 6d7ab29c-7da9-488c-a319-0f4219de7b1c
      group: dev
      userName: nacos
      password: nacos
